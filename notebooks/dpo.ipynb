{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d3de31-b8ac-4321-a675-378022e42fa8",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023847c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip uninstall unsloth xformers torch torchvision torchaudio -y\n",
    "!pip cache purge\n",
    "\n",
    "!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer wandb\n",
    "\n",
    "!pip install transformers==4.51.3 xformers==0.0.28.post1\n",
    "\n",
    "!pip install unsloth[cu124-ampere-torch240]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c2670",
   "metadata": {},
   "source": [
    "# Confirguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Set your tokens here\n",
    "DATASET_REPO = \"somosnlp-hackathon-2025/gastronomia-hispana-dpo\"\n",
    "MODEL_BASE = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\"\n",
    "MODEL_NAME = \"Qwen3-8B-gastronomia-hispana-dpo\"\n",
    "MODEL_REPO = \"somosnlp-hackathon-2025/Qwen3-8B-gastronomia-hispana-dpo\"\n",
    "\n",
    "# Login to Wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210cb138-fae0-4d00-a4f7-9de8b40c76e9",
   "metadata": {},
   "source": [
    "# DPO Trainer patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af1f91-8f54-4495-bd6d-8953574ecb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# One must patch the DPO Trainer first!\n",
    "# import unsloth\n",
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()\n",
    "\n",
    "# from transformers import modeling_utils\n",
    "# if not hasattr(modeling_utils, \"ALL_PARALLEL_STYLES\") or modeling_utils.ALL_PARALLEL_STYLES is None:\n",
    "#     modeling_utils.ALL_PARALLEL_STYLES = [\"tp\", \"none\",\"colwise\",'rowwise']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91604aad",
   "metadata": {},
   "source": [
    "# MODEL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b002b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2500 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_BASE, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    token = os.environ['HF_TOKEN'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a1ed2",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from unsloth import standardize_sharegpt\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Configure tokenizer with ChatML template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"}, # chatml style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "def formatting_dpo_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Standard DPO format: prompt contains system+user, chosen/rejected contain only assistant responses\n",
    "    \"\"\"\n",
    "    chosen_conversations = examples[\"chosen\"]\n",
    "    rejected_conversations = examples[\"rejected\"]\n",
    "    \n",
    "    prompts = []\n",
    "    chosen_texts = []\n",
    "    rejected_texts = []\n",
    "    \n",
    "    print(f\"Processing {len(chosen_conversations)} examples\")\n",
    "    \n",
    "    for i, (chosen_conv, rejected_conv) in enumerate(zip(chosen_conversations, rejected_conversations)):\n",
    "        try:\n",
    "            # Debug: Print first few examples\n",
    "            # if i < 3:\n",
    "            #     print(f\"\\nExample {i}:\")\n",
    "            #     print(f\"Chosen conversation type: {type(chosen_conv)}\")\n",
    "            #     print(f\"Chosen conversation length: {len(chosen_conv)}\")\n",
    "            #     print(f\"First message: {chosen_conv[0] if len(chosen_conv) > 0 else 'Empty'}\")\n",
    "            #     print(f\"Last message: {chosen_conv[-1] if len(chosen_conv) > 0 else 'Empty'}\")\n",
    "            \n",
    "            # Validate conversations\n",
    "            if not chosen_conv or not rejected_conv:\n",
    "                print(f\"Warning: Empty conversation at index {i}\")\n",
    "                prompts.append(\"\")\n",
    "                chosen_texts.append(\"\")\n",
    "                rejected_texts.append(\"\")\n",
    "                continue\n",
    "                \n",
    "            if len(chosen_conv) < 2 or len(rejected_conv) < 2:\n",
    "                print(f\"Warning: Too short conversation at index {i}\")\n",
    "                prompts.append(\"\")\n",
    "                chosen_texts.append(\"\")\n",
    "                rejected_texts.append(\"\")\n",
    "                continue\n",
    "            \n",
    "            # Extract prompt (system + user messages) - should be same for both chosen/rejected\n",
    "            # Take all messages except the last assistant response\n",
    "            prompt_messages = chosen_conv[:-1]  # All except last assistant message\n",
    "            \n",
    "            # Validate that the last message is from assistant\n",
    "            if chosen_conv[-1].get(\"role\") != \"assistant\" or rejected_conv[-1].get(\"role\") != \"assistant\":\n",
    "                print(f\"Warning: Last message is not from assistant at index {i}\")\n",
    "                prompts.append(\"\")\n",
    "                chosen_texts.append(\"\")\n",
    "                rejected_texts.append(\"\")\n",
    "                continue\n",
    "            \n",
    "            # Extract just the assistant responses\n",
    "            chosen_assistant_msg = chosen_conv[-1][\"content\"]  # Last message content\n",
    "            rejected_assistant_msg = rejected_conv[-1][\"content\"]  # Last message content\n",
    "            \n",
    "            # Format prompt (system + user with generation prompt)\n",
    "            prompt_text = tokenizer.apply_chat_template(\n",
    "                prompt_messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            prompts.append(prompt_text)\n",
    "            chosen_texts.append(chosen_assistant_msg)\n",
    "            rejected_texts.append(rejected_assistant_msg)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing conversation {i}: {e}\")\n",
    "            print(f\"Chosen conv structure: {chosen_conv}\")\n",
    "            print(f\"Rejected conv structure: {rejected_conv}\")\n",
    "            # Add empty strings to maintain list length consistency\n",
    "            prompts.append(\"\")\n",
    "            chosen_texts.append(\"\")\n",
    "            rejected_texts.append(\"\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed:\")\n",
    "    print(f\"- Prompts: {len([p for p in prompts if p])}\")\n",
    "    print(f\"- Chosen: {len([c for c in chosen_texts if c])}\")\n",
    "    print(f\"- Rejected: {len([r for r in rejected_texts if r])}\")\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompts,\n",
    "        \"chosen\": chosen_texts, \n",
    "        \"rejected\": rejected_texts\n",
    "    }\n",
    "\n",
    "# Load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\n",
    "    DATASET_REPO, \n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "print(\"Sample data structure:\")\n",
    "print(\"Keys:\", dataset.column_names)\n",
    "print(\"\\nSample chosen conversation:\")\n",
    "print(dataset[0][\"chosen\"])\n",
    "\n",
    "# Formatting approach:\n",
    "\n",
    "# DPO formatting (for preference training)\n",
    "print(\"\\n=== DPO FORMATTING ===\")\n",
    "dpo_dataset = dataset.map(formatting_dpo_prompts_func, batched=True)\n",
    "# Remove original columns\n",
    "columns_to_remove = [col for col in dataset.column_names if col not in [\"prompt\", \"chosen\", \"rejected\"]]\n",
    "dpo_dataset = dpo_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "print(\"DPO formatted example:\")\n",
    "print(\"Prompt:\", dpo_dataset[0][\"prompt\"][:200] + \"...\")\n",
    "print(\"Chosen:\", dpo_dataset[0][\"chosen\"][:200] + \"...\")\n",
    "print(\"Rejected:\", dpo_dataset[0][\"rejected\"][:200] + \"...\")\n",
    "\n",
    "# Split datasets\n",
    "dpo_split = dpo_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "print(f\"\\nDataset splits created:\")\n",
    "print(f\"DPO - Train: {len(dpo_split['train'])}, Eval: {len(dpo_split['test'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbf8e4",
   "metadata": {},
   "source": [
    "# LORA CONFIGURATION\n",
    "\n",
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d96bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                      \"lm_head\"],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Currently only supports dropout = 0\n",
    "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9bde4",
   "metadata": {},
   "source": [
    "# DPO TRAINING CONFIGURATION\n",
    "\n",
    "Now let's use Huggingface TRL's DPOTrainer! More docs here: TRL DPO docs. We do 3 epochs on 0.5% of the dataset to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5918938-7164-46af-86a6-cfc5204ec5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Initialize Wandb run\n",
    "wandb.init(\n",
    "    project=\"somosnpl\",\n",
    "    name=MODEL_NAME,\n",
    "    config={\n",
    "        \"model\": MODEL_BASE,\n",
    "        \"dataset\": DATASET_REPO,\n",
    "        \"technique\": \"DPO\",\n",
    "        \"max_length\": max_seq_length,\n",
    "        \"lora_r\": 64,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"learning_rate\": 5e-7,\n",
    "        \"num_epochs\": 1.5,\n",
    "        \"beta\": 0.5,\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    tokenizer = tokenizer,\n",
    "    beta = 0.8,\n",
    "    train_dataset=dpo_split['train'],\n",
    "    eval_dataset=dpo_split['test'],\n",
    "    max_length = 2500,\n",
    "    max_prompt_length = 350,\n",
    "    dataset_num_proc = 4,\n",
    "    # resume_from_checkpoint=\"outputs/checkpoint-200\",\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 4,\n",
    "        per_device_eval_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_ratio = 0.03,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-7,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        optim = \"adamw_8bit\", # adamw_8bit / adamw_torch_fused\n",
    "        weight_decay = 0.1,\n",
    "        max_grad_norm=0.5,\n",
    "        lr_scheduler_type = \"cosine\", # linear/cosine\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        logging_dir = \"outputs/logs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        run_name = MODEL_NAME,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps = 20,\n",
    "        logging_first_step=True,\n",
    "\n",
    "        # Evaluation\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps=50,\n",
    "        \n",
    "        # Saving\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        # Performance\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        ddp_find_unused_parameters=False,\n",
    "    \n",
    "        # Data loading\n",
    "        dataloader_num_workers=2,\n",
    "        dataloader_pin_memory=True,\n",
    "        remove_unused_columns=True,\n",
    "        dataloader_drop_last = True,  # Consistent batch sizes\n",
    "        prediction_loss_only = False,  # Reduce eval memory usage\n",
    "        \n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fab73",
   "metadata": {},
   "source": [
    "### MEMORY STATS (PRE-TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98deb4",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training...\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74866258-cd94-452d-ac7b-d3f4c58d38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest_checkpoint='./outputs/checkpoint-210'\n",
    "# trainer_stats = trainer.train(resume_from_checkpoint=latest_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb697919",
   "metadata": {},
   "source": [
    "# MEMORY STATS (POST-TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "# Log final stats to Wandb\n",
    "wandb.log({\n",
    "    \"training_time_minutes\": round(trainer_stats.metrics['train_runtime']/60, 2),\n",
    "    \"peak_memory_gb\": used_memory,\n",
    "    \"training_memory_gb\": used_memory_for_lora,\n",
    "    \"memory_usage_percent\": used_percentage,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5c9da",
   "metadata": {},
   "source": [
    "# INFERENCE TEST\n",
    "Let's run the model! Since we're using ChatML, use apply_chat_template with add_generation_prompt set to True for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, apply the compatibility fix that was in your original notebook\n",
    "# from transformers import modeling_utils\n",
    "# if not hasattr(modeling_utils, \"ALL_PARALLEL_STYLES\") or modeling_utils.ALL_PARALLEL_STYLES is None:\n",
    "#     modeling_utils.ALL_PARALLEL_STYLES = [\"tp\", \"none\", \"colwise\", \"rowwise\"]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING INFERENCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"}, # chatml style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Test with a food-related question in Spanish\n",
    "messages = [\n",
    "    # {\"role\": \"system\",\"content\": \"Eres un maestro culinario especializado en tÃ©cnicas de cocciÃ³n internacionales, con expertise en tiempos, temperaturas y mÃ©todos tradicionales de diversas culturas gastronÃ³micas. responde siempre en espaÃ±ol \\no-think\"},\n",
    "    {\"role\": \"user\", \"content\": \"Â¿PodrÃ­as explicarme paso a paso cÃ³mo preparar encebollado ecuatorianos? \\no-think\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Use a TextStreamer for continuous inference\n",
    "# so you can see the generation token by token\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "print(\"Model response:\")\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 2048, use_cache = True)\n",
    "\n",
    "# waiting the whole time!\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "# tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed57a0",
   "metadata": {},
   "source": [
    "# SAVE TO HUGGING FACE - fine-tuning model\n",
    "\n",
    "To save the final model as LoRA adapters, either use Huggingface's push_to_hub for an online save or save_pretrained for a local save.\n",
    "\n",
    "[NOTE] This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save the model and tokenizer locally\n",
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8bd2df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Test the local lora model\n",
    "Now if you want to load the LoRA adapters we just saved for inference, set False to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2500 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"./lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Â¿CuÃ¡les son los ingredientes principales del Motepillo?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 4096, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918dbd4",
   "metadata": {},
   "source": [
    "You can also use Hugging Face's AutoModelForPeftCausalLM. Only use this if you do not have unsloth installed. It can be hopelessly slow, since 4bit model downloading is not supported, and Unsloth's inference is 2x faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoModelForPeftCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = AutoModelForPeftCausalLM.from_pretrained(\n",
    "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8763b",
   "metadata": {},
   "source": [
    "# Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to float16 directly. Select merged_16bit for float16 or merged_4bit for int4. We also allow lora adapters as a fallback. Use push_to_hub_merged to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e31029",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution 1: Comprehensive DTensor Patch\n",
    "\n",
    "# Add this code before trying to save the merged model\n",
    "def patch_dtensor_comprehensive():\n",
    "    \"\"\"Comprehensive patch for DTensor issues in transformers\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        from transformers import pytorch_utils, modeling_utils\n",
    "        import types\n",
    "        \n",
    "        # Create a dummy DTensor class\n",
    "        class DummyDTensor:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "            def to_local(self):\n",
    "                return self\n",
    "        \n",
    "        # Ensure torch.distributed.tensor module exists\n",
    "        if not hasattr(torch.distributed, 'tensor'):\n",
    "            torch.distributed.tensor = types.ModuleType('tensor')\n",
    "        \n",
    "        # Add DTensor to the module\n",
    "        torch.distributed.tensor.DTensor = DummyDTensor\n",
    "        \n",
    "        # Patch id_tensor_storage function\n",
    "        original_id_tensor_storage = pytorch_utils.id_tensor_storage\n",
    "        \n",
    "        def patched_id_tensor_storage(tensor):\n",
    "            try:\n",
    "                return original_id_tensor_storage(tensor)\n",
    "            except (ImportError, NameError):\n",
    "                if hasattr(tensor, 'data_ptr'):\n",
    "                    return tensor.data_ptr()\n",
    "                else:\n",
    "                    return id(tensor)\n",
    "        \n",
    "        pytorch_utils.id_tensor_storage = patched_id_tensor_storage\n",
    "        \n",
    "        # Patch the save_pretrained method to handle DTensor references\n",
    "        if hasattr(modeling_utils, 'PreTrainedModel'):\n",
    "            original_save_pretrained = modeling_utils.PreTrainedModel.save_pretrained\n",
    "            \n",
    "            def patched_save_pretrained(self, *args, **kwargs):\n",
    "                # Add DTensor to globals temporarily\n",
    "                old_globals = modeling_utils.__dict__.copy()\n",
    "                modeling_utils.DTensor = DummyDTensor\n",
    "                \n",
    "                try:\n",
    "                    return original_save_pretrained(self, *args, **kwargs)\n",
    "                finally:\n",
    "                    # Restore original globals\n",
    "                    modeling_utils.__dict__.clear()\n",
    "                    modeling_utils.__dict__.update(old_globals)\n",
    "            \n",
    "            modeling_utils.PreTrainedModel.save_pretrained = patched_save_pretrained\n",
    "        \n",
    "        # Also patch any other modules that might reference DTensor\n",
    "        import transformers\n",
    "        if not hasattr(transformers, 'DTensor'):\n",
    "            transformers.DTensor = DummyDTensor\n",
    "        \n",
    "        print(\"âœ… Comprehensive DTensor patch applied successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Comprehensive patching failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Apply the comprehensive patch\n",
    "# patch_dtensor_comprehensive()\n",
    "\n",
    "# Merge to 16bit\n",
    "if False:\n",
    "    print(\"Saving merged model in float16...\")\n",
    "    model.save_pretrained_merged(f\"{MODEL_REPO}\", tokenizer, save_method = \"merged_16bit\",)\n",
    "    model.push_to_hub_merged(f\"{MODEL_REPO}\", tokenizer, save_method = \"merged_16bit\", token = os.environ['HF_TOKEN'])\n",
    "    print(f\"âœ… Float16 model saved to: {MODEL_REPO}\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if True:\n",
    "    print(\"Saving merged model in 4bit...\")\n",
    "    model.save_pretrained_merged(f\"{MODEL_REPO}-4bit\", tokenizer, save_method = \"merged_4bit_forced\",)\n",
    "    model.push_to_hub_merged(f\"{MODEL_REPO}-4bit\", tokenizer, save_method = \"merged_4bit_forced\", token = os.environ['HF_TOKEN'])\n",
    "    print(f\"âœ… Float16 model saved to: {MODEL_REPO}\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    print(\"Saving LoRA adapters...\")\n",
    "    model.save_pretrained_merged(f\"{MODEL_REPO}-LoRA\", tokenizer, save_method = \"lora\",)\n",
    "    model.push_to_hub_merged(f\"{MODEL_REPO}-LoRA\", tokenizer, save_method = \"lora\", token = os.environ['HF_TOKEN'])\n",
    "    print(f\"âœ… LoRA adapters saved to: {MODEL_REPO}-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be2a77-b2ee-4d0c-a88a-bf4a3ee44efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "679be53f",
   "metadata": {},
   "source": [
    "# GGUF / llama.cpp Conversion\n",
    "\n",
    "To save to GGUF / llama.cpp, we support it natively now! We clone llama.cpp and we default save it to q8_0. We allow all methods like q4_k_m. Use save_pretrained_gguf for local saving and push_to_hub_gguf for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our Wiki page):\n",
    "\n",
    "    q8_0 - Fast conversion. High resource use, but generally acceptable.\n",
    "    q4_k_m - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "    q5_k_m - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 16bit GGUF\n",
    "if True: model.save_pretrained_gguf(f\"{MODEL_REPO}-GGUF\", tokenizer, quantization_method = \"f16\")\n",
    "if True: model.push_to_hub_gguf(f\"{MODEL_REPO}-GGUF\", tokenizer, quantization_method = \"f16\", token = os.environ['HF_TOKEN'])\n",
    "\n",
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer, token = os.environ['HF_TOKEN'])\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer, quantization_method = \"q4_k_m\", token = os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f4310-8688-49bb-adf5-d891cf71c8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fef0d-49a1-4dcd-99b2-a7f4da418e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Configuration (adjust these to match your setup)\n",
    "MODEL_BASE = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\"\n",
    "MODEL_REPO = \"somosnlp-hackathon-2025/Qwen3-8B-gastronomia-hispana-dpo\"\n",
    "LORA_PATH = \"./lora_model\"\n",
    "\n",
    "def check_disk_space():\n",
    "    \"\"\"Check available disk space\"\"\"\n",
    "    total, used, free = shutil.disk_usage(\"/data\")\n",
    "    print(f\"ðŸ’¾ Disk Space - Total: {total//1024**3}GB, Used: {used//1024**3}GB, Free: {free//1024**3}GB\")\n",
    "    return free // 1024**3  # Return free space in GB\n",
    "\n",
    "def comprehensive_dtensor_patch():\n",
    "    \"\"\"Fix DTensor issues for model saving\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        from transformers import pytorch_utils, modeling_utils\n",
    "        import types\n",
    "        \n",
    "        # Create dummy DTensor class\n",
    "        class DummyDTensor:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "            def to_local(self):\n",
    "                return self\n",
    "        \n",
    "        # Ensure torch.distributed.tensor module exists\n",
    "        if not hasattr(torch.distributed, 'tensor'):\n",
    "            torch.distributed.tensor = types.ModuleType('tensor')\n",
    "        \n",
    "        torch.distributed.tensor.DTensor = DummyDTensor\n",
    "        \n",
    "        # Patch id_tensor_storage function\n",
    "        original_id_tensor_storage = pytorch_utils.id_tensor_storage\n",
    "        \n",
    "        def patched_id_tensor_storage(tensor):\n",
    "            try:\n",
    "                return original_id_tensor_storage(tensor)\n",
    "            except (ImportError, NameError):\n",
    "                if hasattr(tensor, 'data_ptr'):\n",
    "                    return tensor.data_ptr()\n",
    "                else:\n",
    "                    return id(tensor)\n",
    "        \n",
    "        pytorch_utils.id_tensor_storage = patched_id_tensor_storage\n",
    "        \n",
    "        # Patch save_pretrained method\n",
    "        if hasattr(modeling_utils, 'PreTrainedModel'):\n",
    "            original_save_pretrained = modeling_utils.PreTrainedModel.save_pretrained\n",
    "            \n",
    "            def patched_save_pretrained(self, *args, **kwargs):\n",
    "                old_globals = modeling_utils.__dict__.copy()\n",
    "                modeling_utils.DTensor = DummyDTensor\n",
    "                \n",
    "                try:\n",
    "                    return original_save_pretrained(self, *args, **kwargs)\n",
    "                finally:\n",
    "                    modeling_utils.__dict__.clear()\n",
    "                    modeling_utils.__dict__.update(old_globals)\n",
    "            \n",
    "            modeling_utils.PreTrainedModel.save_pretrained = patched_save_pretrained\n",
    "        \n",
    "        import transformers\n",
    "        if not hasattr(transformers, 'DTensor'):\n",
    "            transformers.DTensor = DummyDTensor\n",
    "        \n",
    "        print(\"âœ… DTensor patch applied successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ DTensor patching failed: {e}\")\n",
    "\n",
    "\n",
    "print(\"ðŸŽ¯ BFloat-16 Model Creation with Space Optimization\")\n",
    "print(\"=\" * 60)\n",
    "    \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=LORA_PATH,\n",
    "    max_seq_length=2500,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,  # Load in 4bit to save memory\n",
    ")\n",
    "\n",
    "# Configure tokenizer\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\"role\": \"role\", \"content\": \"content\", \"user\": \"user\", \"assistant\": \"assistant\"},\n",
    "    map_eos_token=True,\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded successfully\")\n",
    "# check_disk_space()\n",
    "\n",
    "print(\"\\nðŸ’¾ Creating BFloat-16 merged model...\")\n",
    "\n",
    "# Option 1: Save locally first (if you have space)\n",
    "# local_save = True\n",
    "# if local_save and free_space >= 18:\n",
    "#     print(\"Saving locally first...\")\n",
    "#     model.save_pretrained_merged(\n",
    "#         f\"{MODEL_REPO}-local\", \n",
    "#         tokenizer, \n",
    "#         save_method=\"merged_16bit\"\n",
    "#     )\n",
    "#     print(\"âœ… Local BFloat-16 model saved\")\n",
    "#     check_disk_space()\n",
    "\n",
    "# Option 2: Push directly to HuggingFace (recommended for space saving)\n",
    "print(\"\\nðŸš€ Uploading to HuggingFace...\")\n",
    "model.push_to_hub_merged(\n",
    "    MODEL_REPO, \n",
    "    tokenizer, \n",
    "    save_method=\"merged_16bit\", \n",
    "    token=os.environ.get('HF_TOKEN')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40a7d5-558c-43de-84b9-f2a571d67ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
