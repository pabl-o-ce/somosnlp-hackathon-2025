{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d3de31-b8ac-4321-a675-378022e42fa8",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023847c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "!pip install --upgrade pip\n",
    "\n",
    "!pip uninstall unsloth torch torchvision torchaudio xformers -y\n",
    "!pip cache purge\n",
    "\n",
    "!pip install torch==2.4.* torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "# !pip install --no-deps bitsandbytes accelerate peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "# !pip install --no-deps xformers==0.0.28.post3\n",
    "\n",
    "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer wandb\n",
    "\n",
    "#!pip install unsloth\n",
    "#!pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "!pip install \"unsloth[cu124-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c2670",
   "metadata": {},
   "source": [
    "# Confirguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your tokens here\n",
    "MODEL_NAME = \"somosnlp-hackathon-2025/mistral-small-tortuga-galapagos\"  # Change to your desired model name\n",
    "# Login to Wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91604aad",
   "metadata": {},
   "source": [
    "# MODEL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b002b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = os.environ['HF_TOKEN'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbf8e4",
   "metadata": {},
   "source": [
    "# LORA CONFIGURATION\n",
    "\n",
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d96bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a1ed2",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c528eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from unsloth import standardize_sharegpt\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"}, # chatml style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = []\n",
    "    \n",
    "    for convo in convos:\n",
    "        try:\n",
    "            # Parse the JSON string to get the actual conversation list\n",
    "            if isinstance(convo, str):\n",
    "                parsed_convo = standardize_sharegpt(json.loads(convo))\n",
    "            else:\n",
    "                parsed_convo = convo\n",
    "            \n",
    "            if isinstance(parsed_convo, list) and len(parsed_convo) >= 2:\n",
    "                if len(parsed_convo) == 2:\n",
    "                    texts.append(tokenizer.apply_chat_template(parsed_convo, tokenize=False, add_generation_prompt=False))\n",
    "                # else:\n",
    "                #     chunks = [parsed_convo[i:i+2] for i in range(0, len(parsed_convo), 2)]\n",
    "                #     for chunk in chunks:\n",
    "                #         # print(\"line\\n\\n\")\n",
    "                #         # print(chunk)\n",
    "                #         # print(\"enline\\n\\n\")\n",
    "                #         texts.append(tokenizer.apply_chat_template(chunk, tokenize=False, add_generation_prompt=False))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing conversation {i}: {e}\")\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "pass\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"somosnlp-hackathon-2025/Patrimonio-Gastronomico-Colombiano-Ecuatoriano\", split = \"train\")\n",
    "# dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# Load the specific dataset file you requested\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\n",
    "    \"somosnlp-hackathon-2025/Patrimonio-Gastronomico-Colombiano-Ecuatoriano\", \n",
    "    data_files=\"data/train-somosnpl-recetas-zero-v2.parquet\",\n",
    "    split = \"train\"\n",
    ")\n",
    "\n",
    "# First, collect all the processed texts\n",
    "dataset = dataset.filter(lambda example: example['metadata.type'] != 'multi_turn')\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "print(\"Sample data structure:\")\n",
    "print(dataset[5])\n",
    "\n",
    "# Create a new dataset from the processed texts\n",
    "# from datasets import Dataset\n",
    "# dataset = Dataset.from_dict({\"text\": all_processed_data[\"text\"]})\n",
    "# dataset = dataset.filter(lambda x: x[\"text\"] is not None and len(x[\"text\"].strip()) > 0)\n",
    "# dataset = dataset.remove_columns([col for col in dataset.column_names if col != \"text\"])\n",
    "# Apply formatting\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset = dataset.remove_columns([col for col in dataset.column_names if col != \"text\"])\n",
    "\n",
    "# Let's see how the ChatML format works by printing the first element\n",
    "print(\"\\nFormatted conversation example:\")\n",
    "print(dataset[5])\n",
    "\n",
    "# Split dataset (90% train, 10% validation)\n",
    "dataset_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9bde4",
   "metadata": {},
   "source": [
    "# TRAINING CONFIGURATION\n",
    "\n",
    ">>> Now let's use Huggingface TRL's SFTTrainer! More docs here: TRL SFT docs. We do 60 steps to speed things up, but you can set num_train_epochs=1 for a full run, and turn off max_steps=None. We also support TRL's DPOTrainer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Initialize Wandb run\n",
    "wandb.init(\n",
    "    project=\"somosnpl\",\n",
    "    name=\"mistral-7b-tortuga-galapagos\",\n",
    "    config={\n",
    "        \"model\": \"mistral-7b-instruct\",\n",
    "        \"dataset\": \"Patrimonio-Gastronomico-Colombiano-Ecuatoriano\",\n",
    "        \"technique\": \"QLoRA\",\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = True, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 100,\n",
    "        num_train_epochs = 2,  # Changed from max_steps to full epoch\n",
    "        # max_steps = 60,  # Commented out to allow full training\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        optim = \"adamw_torch_fused\", # adamw_8bit / adamw_torch_fused\n",
    "        weight_decay = 0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type = \"cosine\", # linear/cosine\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        run_name = \"mistral-7b-tortuga-galapagos\",\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps = 25,\n",
    "        logging_first_step=True,\n",
    "\n",
    "        # Evaluation\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps=100,\n",
    "        \n",
    "        # save_strategy = \"epoch\",\n",
    "        # save_total_limit = 2,\n",
    "        # Saving\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        # Performance\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        ddp_find_unused_parameters=False,\n",
    "    \n",
    "        # Data loading\n",
    "        # dataloader_num_workers=4,\n",
    "        # remove_unused_columns=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fab73",
   "metadata": {},
   "source": [
    "### MEMORY STATS (PRE-TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98deb4",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Señores y señoras arranca el entrenamiento\\nStart training...\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb697919",
   "metadata": {},
   "source": [
    "# MEMORY STATS (POST-TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "\n",
    "# Log final stats to Wandb\n",
    "wandb.log({\n",
    "    \"training_time_minutes\": round(trainer_stats.metrics['train_runtime']/60, 2),\n",
    "    \"peak_memory_gb\": used_memory,\n",
    "    \"training_memory_gb\": used_memory_for_lora,\n",
    "    \"memory_usage_percent\": used_percentage,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5c9da",
   "metadata": {},
   "source": [
    "# INFERENCE TEST\n",
    "Let's run the model! Since we're using ChatML, use apply_chat_template with add_generation_prompt set to True for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ca6bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING INFERENCE\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unsloth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTESTING INFERENCE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_templates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_chat_template\n\u001b[32m      7\u001b[39m tokenizer = get_chat_template(\n\u001b[32m      8\u001b[39m     tokenizer,\n\u001b[32m      9\u001b[39m     chat_template = \u001b[33m\"\u001b[39m\u001b[33mchatml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\u001b[39;00m\n\u001b[32m     10\u001b[39m     mapping = {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m}, \u001b[38;5;66;03m# chatml style\u001b[39;00m\n\u001b[32m     11\u001b[39m     map_eos_token = \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Maps <|im_end|> to </s> instead\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m FastLanguageModel.for_inference(model) \u001b[38;5;66;03m# Enable native 2x faster inference\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'unsloth'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING INFERENCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"role\", \"content\" : \"content\", \"user\" : \"user\", \"assistant\" : \"assistant\"}, # chatml style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Test with a food-related question in Spanish\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuáles son los ingredientes principales del encebollado ecuatoriano?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Use a TextStreamer for continuous inference\n",
    "# so you can see the generation token by token\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "print(\"Model response:\")\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 256, use_cache = True)\n",
    "\n",
    "# waiting the whole time!\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "# tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed57a0",
   "metadata": {},
   "source": [
    "# SAVE TO HUGGING FACE - fine-tuning model\n",
    "\n",
    "To save the final model as LoRA adapters, either use Huggingface's push_to_hub for an online save or save_pretrained for a local save.\n",
    "\n",
    "[NOTE] This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save the model and tokenizer locally\n",
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8bd2df",
   "metadata": {},
   "source": [
    "# Test the local lora model\n",
    "Now if you want to load the LoRA adapters we just saved for inference, set False to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuáles son los ingredientes principales del Motepillo?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 4096, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918dbd4",
   "metadata": {},
   "source": [
    "You can also use Hugging Face's AutoModelForPeftCausalLM. Only use this if you do not have unsloth installed. It can be hopelessly slow, since 4bit model downloading is not supported, and Unsloth's inference is 2x faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoModelForPeftCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = AutoModelForPeftCausalLM.from_pretrained(\n",
    "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8763b",
   "metadata": {},
   "source": [
    "# Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to float16 directly. Select merged_16bit for float16 or merged_4bit for int4. We also allow lora adapters as a fallback. Use push_to_hub_merged to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e31029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "print(\"Saving merged model in float16...\")\n",
    "if False: model.save_pretrained_merged(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas\", tokenizer, save_method = \"merged_16bit\", token = os.environ['HF_TOKEN'])\n",
    "print(f\"✅ Float16 model saved to: {MODEL_NAME}\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-4bit\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-4bit\", tokenizer, save_method = \"merged_4bit\", token = os.environ['HF_TOKEN'])\n",
    "\n",
    "# Just LoRA adapters\n",
    "print(\"Saving LoRA adapters...\")\n",
    "if False: model.save_pretrained_merged(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-LoRA\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-LoRA\", tokenizer, save_method = \"lora\", token = os.environ['HF_TOKEN'])\n",
    "print(f\"✅ LoRA adapters saved to: {MODEL_NAME}-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679be53f",
   "metadata": {},
   "source": [
    "# GGUF / llama.cpp Conversion\n",
    "\n",
    "To save to GGUF / llama.cpp, we support it natively now! We clone llama.cpp and we default save it to q8_0. We allow all methods like q4_k_m. Use save_pretrained_gguf for local saving and push_to_hub_gguf for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our Wiki page):\n",
    "\n",
    "    q8_0 - Fast conversion. High resource use, but generally acceptable.\n",
    "    q4_k_m - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "    q5_k_m - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 16bit GGUF\n",
    "if True: model.save_pretrained_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer, quantization_method = \"f16\")\n",
    "if True: model.push_to_hub_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer, quantization_method = \"f16\", token = os.environ['HF_TOKEN'])\n",
    "\n",
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer, token = os.environ['HF_TOKEN'])\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(f\"somosnlp-hackathon-2025/mistral-7B-ec-es-recetas-GGUF\", tokenizer, quantization_method = \"q4_k_m\", token = os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f4310-8688-49bb-adf5-d891cf71c8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
